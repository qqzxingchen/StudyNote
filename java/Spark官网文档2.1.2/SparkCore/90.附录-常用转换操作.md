# 常用转换操作


## map(func)

* 新 rdd 中的每个元素都是原 rdd 中相应位置的元素通过 func 方法计算得到的

```bash
>>> distData = sc.parallelize([1, 2, 3, 4, 5])
>>> distData.map(lambda x: x+10).collect()
[11, 12, 13, 14, 15]
```

## filter(func)

* 原 rdd 中每个元素都传给 func，只有计算结果为 true 的元素才会被放入新 rdd

```bash
>>> distData = sc.parallelize([1, 2, 3, 4, 5])
>>> distData.filter( lambda x: x%2==0 ).collect()
[2, 4]
```

## flatMap(func)

* 类似于 `map` 但是原 rdd 中的每个元素经过 func 方法处理后，可以返回多个返回值
    * 注意，该方法必须返回集合或者迭代器，否则会抛出异常

```bash
>>> distData = sc.parallelize([1, 2, 3, 4, 5])
>>> distData.flatMap( lambda x:[x,x+100,x+1000] ).collect()
[1, 101, 1001, 2, 102, 1002, 3, 103, 1003, 4, 104, 1004, 5, 105, 1005]
```

* 另外，注意下面的区别

```bash
>>> distData = sc.parallelize([1, 2, 3, 4, 5])
>>> distData.flatMap( lambda x:[x,x+100,x+1000] ).collect()
[1, 101, 1001, 2, 102, 1002, 3, 103, 1003, 4, 104, 1004, 5, 105, 1005]
>>> distData.map( lambda x:[x,x+100,x+1000] ).collect()
[[1, 101, 1001], [2, 102, 1002], [3, 103, 1003], [4, 104, 1004], [5, 105, 1005]]
```

## mapPartitions(func)

* 该函数和 map 函数类似，只不过映射函数的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器。
    * 如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的多。

* 举例，如果要把 rdd 中的每个元素都写入数据库，那么
    * 使用 map 需要为元素的写入建立 jdbc 连接
    * 使用 mapPartitions 只需要为每个区块的写入建立 jdbc 连接

```python
# 将生成的 0~99 的序列分成 5 个分片
data = sc.parallelize(range(100),5)

# 处理函数
def func(partition):
    par_list = list(partition)  # x 的类型为 itertools.chain，python中可以直接转换为 list
    print par_list
    return [sum(par_list)]      # 必须返回一个集合或者迭代器，而不能直接返回一个值

# 在每个分片上应用 func 函数
# 最终，每个分片上的计算结果将会汇总到一起生成一个新的 rdd
print data.mapPartitions( func ).collect()
```

```bash
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
[190, 590, 990, 1390, 1790]
```

## mapPartitionsWithIndex(func)

* 函数作用同 mapPartitions，不过提供了两个参数，第一个参数为分区的索引。

```python
def func(index,partition):
    ...
```

## sample(withReplacement, fraction, seed)

* 对原 rdd 进行抽样

```python
data = sc.parallelize(range(100))

# sample( withReplacement, fraction, seed=None) 
# withReplacement
#       True 表示有放回的抽样 (即即使原 rdd 不存在重复数据，抽样结果也可能存在重复数据)
#       False 表示无放回的抽样 (即如果原 rdd 不存在重复数据，则抽样结果肯定不存在重复数据)
# fraction  一个浮点数，该数表示抽样的概率。比如 fraction = 0.5 ，则表示 rdd 中每个元素都有 50% 的概率被抽取出来
# seed      一个整形，表示抽样的随机数的种子
#           即对于同一 rdd 以及相同的 seed ，则抽样结果一定一样
print data.sample( False,0.9,9 ).collect()
```

## union(otherDataset)

* 将两个 rdd 做并集，注意，不会去重

```python
data1 = sc.parallelize(range(100),5)
data2 = sc.parallelize(range(10),5)
print data1.union( data2 ).collect()
```

```bash
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```

## intersection(otherDataset)

* 只返回两个 RDD 中都有的元素，同时保证返回的 RDD 中不包含重复的元素

```bash
>>> data1 = sc.parallelize([1,2,3,4,5])
>>> data2 = sc.parallelize([2,3,2,3,4])
>>> data1.intersection( data2 ).collect()
[2, 4, 3]
```

## distinct([numTasks]))

* 去除原 rdd 中重复的元素

```bash
>>> data = sc.parallelize([2,3,2,3,4])
>>> data.distinct().collect()
[2, 4, 3]
```

## groupByKey([numTasks])

* 针对键值对进行处理，将具有相同 key 的元素汇总起来

```bash
>>> rdd = sc.parallelize([("a", 10), ("b", 11), ("a", 12)])
>>> rdd.groupByKey().collect()
[('a', <pyspark.resultiterable.ResultIterable object at 0x7fd9b3052690>), ('b', <pyspark.resultiterable.ResultIterable object at 0x7fd9b3052610>)]
>>> rdd.groupByKey().mapValues( lambda l: list(l) ).collect()
[('a', [10, 12]), ('b', [11])]
```

## reduceByKey(func, [numTasks])

* 针对键值对进行处理，将具有相同 key 的元素汇总起来，并通过 func 函数进行汇总计算

```bash
>>> rdd = sc.parallelize([("a", 10), ("b", 11), ("a", 12)])
>>> rdd.reduceByKey(lambda a,b: a+b+10).collect()
[('a', 32), ('b', 11)]
# 以上，观察即可发现，将相同 key 汇总后，如果包含 n 个元素，则将会执行 n-1 次 func
```

## aggregateByKey(zeroValue, seqOp, comOp, [numTasks])

* 将每个分区里面的元素进行聚合，然后用 combine 函数将每个分区的结果和初始值 zeroValue 进行 combine 操作。
    这个函数最终返回的类型不需要和RDD中元素类型一致。

* seqOp操作会聚合各分区中的元素，然后combOp操作把所有分区的聚合结果再次聚合，两个操作的初始值都是zeroValue
    * seqOp 的操作是遍历分区中的所有元素 T，第一个 T 跟 zeroValue 做操作，结果再作为与第二个 T 做操作的 zeroValue ，直到遍历完整个分区。
    * comOp 操作是把各分区聚合的结果，再聚合。

* 注意事项
    * `zeroValue` ，该值并非表示一个 0 值，而是代表一个 对操作值不变的值。
        比如对于乘法来说， `1` 就是 `zeroValue` ，对于加法来说， `0` 就是一个 `zeroValue`
    * 如果传入的 `zeroValue` 不满足上面所说的条件，则计算结果可能不可预知

```python
# 分区内聚合
# 选定某分区，先把 zeroValue 与分区内第一个数使用 seqOp 进行操作，然后再把结果和分区内第二个数使用 seqOp 进行操作，直到分区所有数据全部聚合完毕
# 接受参数: x,y
#       x: zeroValue 或者上一次聚合的结果
#       y: 分区内的每个数
def seqOp(x,y):
    print 'seqOp',x,y
    return ( x[0]*y,x[1]+1 )

# 分区间聚合，等到各个分区的数据全部都各自聚合完毕之后，启动整体汇总
# 仍然是先把 zeroValue 与第一个分区聚合结果使用 comOp 进行操作，然后再把结果与第二个分区的聚合结果使用 comOp 进行操作，直到所有分区的结果都聚合完毕
# 接受参数: x,y
#       x: zeroValue 或者上一次聚合的结果
#       y: 每个分区的聚合结果
def comOp(x,y):
    print 'comOp',x,y
    return ( x[0]*y[0],x[1]+y[1] )

# 该样例程序实现的是阶乘，对于乘法操作来说， zeroValue 就是 1
# 这里之所以传入 (1,0) ，其中 1 代表 zeroValue ， 0 代表累乘次数
print sc.parallelize(range(1,20),3).aggregate( (1,0),seqOp,comOp )
```

```bash
# 第一个分区的 seqOp 聚合
seqOp (1, 0) 1
seqOp (1, 1) 2
seqOp (2, 2) 3
seqOp (6, 3) 4
seqOp (24, 4) 5
seqOp (120, 5) 6
# 第二个分区的 seqOp 聚合
seqOp (1, 0) 7
seqOp (7, 1) 8
seqOp (56, 2) 9
seqOp (504, 3) 10
seqOp (5040, 4) 11
seqOp (55440, 5) 12
# 第三个分区的 seqOp 聚合
seqOp (1, 0) 13
seqOp (13, 1) 14
seqOp (182, 2) 15
seqOp (2730, 3) 16
seqOp (43680, 4) 17
seqOp (742560, 5) 18
seqOp (13366080, 6) 19
# 各分区结果的 comOp 聚合
comOp (1, 0) (720, 6)
comOp (720, 6) (665280, 6)
comOp (479001600, 12) (253955520, 7)
# 最终结果
(121645100408832000, 19)
```

## sortByKey([ascending], [numTasks])

* 针对键值对 rdd ，根据 key 进行排序

```python
data = []
for i in range(10):
    j = 100-i
    data.append( (j,1) )
print data
# sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)
# 可以通过 keyfunc 来指定 key 的转换函数（比如 'a' 转换成 97，或者 id 号转换为对应记录的英文名等）
#       该函数的接口参数 x 表示的时键值对中的 key ，因此不必再使用 x[0] 来获取 key
print sc.parallelize(data).sortByKey().collect()
print sc.parallelize(data).sortByKey(keyfunc=lambda x: 100-x).collect()
```

```bash
[(100, 1), (99, 1), (98, 1), (97, 1), (96, 1), (95, 1), (94, 1), (93, 1), (92, 1), (91, 1)]
[(91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1)]
[(100, 1), (99, 1), (98, 1), (97, 1), (96, 1), (95, 1), (94, 1), (93, 1), (92, 1), (91, 1)]
```

## join(otherDataset, [numTasks])

* join 函数会输出两个 RDD 中 key 相同的所有项，并将它们的 value 联结起来，它联结的 key 要求在两个表中都存在，类似于 SQL 中的 INNER JOIN。
    但它不满足交换律，a.join(b) 与 b.join(a) 的结果不完全相同，值插入的顺序与调用关系有关。

```python
data1 = [
    (1,2),
    (1,3),
    (2,2),
    (3,2),
]
data2 = [
    (1,3),
    (1,3),
    (2,3),
    (2,4),
    (5,3),
    (6,3),
]
print sc.parallelize(data1).join(sc.parallelize(data2)).collect()
```

```bash
# 会发现， data1 和 data2 相同 key 包括， 1,2
# 分别为:
#       data1:  (1,2)(1,3)(2,2)
#       data2:  (1,3)(1,3)(2,3)(2,4)
# key 相同的分别汇总，因此结果如下
[(2, (2, 3)), (2, (2, 4)), (1, (2, 3)), (1, (2, 3)), (1, (3, 3)), (1, (3, 3))]
```

## cogroup(otherDataset, [numTasks])

* 将多个 RDD 中同一个 key 对应的 value 组合到一起

```python
data1 = [
    (1,2),
    (1,3),
    (2,2),
    (3,2),
]
data2 = [
    (2,3),
    (2,4),
    (1,3),
    (1,3),
    (5,3),
    (6,3),
]
x = sc.parallelize(data1)
y = sc.parallelize(data2)
[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]
```

```bash
# 会发现， cogroup 会将每个 rdd 中的所有 key 相同的 value 汇聚到一起，然后再把不同 rdd 中 key 相同的 value 集合汇聚到一起
#       如果某 key 只在一个 rdd 中出现，则结果中将会使用 [] 来占位
[(1, ([2, 3], [3, 3])), (2, ([2], [3, 4])), (3, ([2], [])), (5, ([], [3])), (6, ([], [3]))]
```

## cartesian(otherDataset)

* 求取 rdd 到 otherDataset 的笛卡尔积

```python
x = sc.parallelize([ (1,2),(1,3) ])
y = sc.parallelize([ (2,3),(2,4),(1,3) ])
print x.cartesian(y).collect()
```

```bash
[
    ((1, 2), (2, 3)),
    ((1, 2), (2, 4)),
    ((1, 2), (1, 3)),
    ((1, 3), (2, 3)),
    ((1, 3), (2, 4)),
    ((1, 3), (1, 3))
]
```

## coalesce(numPartitions)

* 将 RDD 的分区重新划分
    * 函数声明 `coalesce(numPartitions, shuffle=False)`
    * `numPartitions`: 新分区个数
    * `shuffle`: 是否进行 shuffle

* 注意，如果 `numPartitions` 大于 rdd 中原来的分区数，则如果 `shuffle==False`，则该重分区操作不生效。
    即如果 `numPartitions` 大于 rdd 中原来的分区数，则 `shuffle==True` 时才会真正地重新规划分区

```bash
>>> data = sc.parallelize( range(100000),10 )
10
>>> data.coalesce(1000).getNumPartitions()
10
>>> data.coalesce(1000,True).getNumPartitions()
1000
```


## repartition(numPartitions)

* 它只是 `coalesce` 的 `shuffle=True` 的简易接口

```python
def repartition(numPartitions):
    return coalesce(numPartitions,shuffle=True)
```

## repartitionAndSortWithinPartitions(partitioner)

* 如果需要在 `repartition` 重分区之后，还要进行排序，建议直接使用 `repartitionAndSortWithinPartitions` 算子。
    因为该算子可以一边进行重分区的 shuffle 操作，一边进行排序。shuffle 与 sort 两个操作同时进行，比先 shuffle 再 sort 来说，性能会好一些

```python
rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
# repartitionAndSortWithinPartitions(
#       numPartitions=None,                         # 新的分区数
#       partitionFunc=<function portable_hash>,     # 分区函数
#       ascending=True,                             # 升序还是降序
#       keyfunc=<function <lambda>>
# )
# 分区函数指定为 lambda x: x%2 ，则 key 为奇数的放到一个分区中，key 为偶数的放到另一个分区中
rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x%2)
rdd2.glom().collect()
```

```bash
[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
```

## pipe(command, [envVars])

* 未知，如果用到了再说
