# Overview

## 简单描述

* 在较高层次上，每个Spark应用程序都包含一个驱动程序，该程序运行用户的主要功能，同时在集群上执行各种并行操作。

* Spark 提供的抽象
    * 弹性分布式数据集（RDD），它是在集群节点间进行分区的元素集合，可以并行操作。
        * RDD可以通过从Hadoop文件系统（或任何其他Hadoop支持的文件系统）中的文件或驱动程序中现有的集合开始创建
        * 可以通过 Spark 提供的各种算子对其进行转换
        * 也可以要求 Spark 将 RDD 保存在内存中，以便在并行操作中有效地重用它。
        * 最后，RDD自动从节点故障中恢复。
    * 可用于并行操作的共享变量
        * 默认情况下， Spark 在不同节点上并行执行一组任务时，会将该函数中使用的每个变量的副本传送给每个任务。这时可能需要在任务之间或者任务与驱动程序之间进行共享
        * Spark 支持两种类型的共享变量
            * 广播变量: 可用于在所有节点上缓存值
            * 累加器: 只提供 `+` 操作，如累加器等

## 使用 Spark

* java 中的使用

```java
// groupId = org.apache.spark
// artifactId = spark-core_2.11
// version = 2.1.2
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;
class Test{
    public static void main(String[] args) {
        // app name 是指应用程序的名字，方便辨识
        // master 是将要链接到的 spark 集群的地址，详细描述: https://spark.apache.org/docs/2.1.2/submitting-applications.html#master-urls
        SparkConf conf = new SparkConf().setAppName("spark test").setMaster("spark://HOST:PORT");
        JavaSparkContext sc = new JavaSparkContext(conf);
        ...
    }
}
```

* python 中的使用

```python
# pip install pyspark==2.1.2
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("spark test").setMaster("spark://HOST:PORT")
sc = SparkContext(conf=conf)
```

```bash
$ ./bin/pyspark --master ${spark_service_url}
$ ./bin/pyspark --master ${spark_service_url} --py-files code.py
```
