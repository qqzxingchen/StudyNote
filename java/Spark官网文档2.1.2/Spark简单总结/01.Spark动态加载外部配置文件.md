# Spark 动态加载外部配置文件

## 代码

```java
public class TestPropLoad {
    public static void main(String[] args) throws Exception {
        SparkConf conf = new SparkConf();
        JavaSparkContext sc = new JavaSparkContext(conf);

        InputStream is = TestPropLoad.class.getResourceAsStream( "/test_load_config.properties" );
        if ( is == null ){
            System.out.println( "/test_load_config.properties is not valid" );
            return;
        }

        Properties prop = new Properties();
        prop.load( is );
        Const.init(prop);
        Const.print();

        JavaRDD<String> input = sc.parallelize(new ArrayList<String>(){{
            for ( int i = 0 ; i < 1000 ; i ++ ){
                add( String.valueOf( i ) );
            }
        }},10);
        input.foreachPartition(new VoidFunction<Iterator<String>>() {
            @Override
            public void call(Iterator<String> stringIterator) throws Exception {
                String finalValue = "-1";
                while ( stringIterator.hasNext() ){
                    finalValue = stringIterator.next();
                }
                System.out.println( finalValue );

                Const.print();
            }
        });
    }
}
```

```java
public class Const {
    private static int xc = -1;
    private static int gp = -1;

    /**
     * Const 对象将会同时存在在 driver 节点和 executor 节点上
     *      driver 节点上的 Const 对象将会在 TestPropLoad.main 中进行初始化
     *      executor 节点上的 Const 对象将会在 static{} 中进行初始化
     * 注意，由于位于 driver 节点上的 Const 对象是在 SparkContext 对象初始化完毕之后才会真正的赋值，
     *      而 Const 的 static{} 区块将会在系统初始化就会执行，因此 static{} 的代码必然会失败一次，因此注意捕获异常
     **/
    static {
        String fileName = SparkFiles.get("test_load_config.properties");
        System.out.println( "SparkFiles.get file exists " + new File(fileName).exists() );
        Properties prop = new Properties();
        try {
            prop.load( new FileInputStream(fileName) );
            init( prop );
        }catch (Exception e){
            e.printStackTrace();
        }
    }

    public static void init( Properties prop ) throws Exception{
        xc = Integer.valueOf(prop.getProperty("dc.xc"));
        gp = Integer.valueOf(prop.getProperty("dc.gp"));
    }

    public static void print(){
        System.out.println( String.format(
                "load success\nxc:%s    gp:%s",Const.xc,Const.gp
        ) );
    }
}
```

```python
##### test_load_config.properties
dc.xc=1
dc.gp=2
```

```bash
spark-submit                                    \
    --name xctest                               \
    --class com.gp.test.TestPropLoad            \
    --master yarn                               \
    --deploy-mode cluster                       \
    --files test_load_config.properties         \
    demo-1.0-jar-with-dependencies.jar
```

## 描述

1. 由于是集群模式(`--deploy-mode cluster`)，因此不能直接在 java 代码中直接读取配置文件 `test_load_config.properties`。
    因为集群模式的情况下， jar 包 main 方法的执行并非在执行 `spark-submit` 命令的节点执行，而是在 driver 节点

2. 通过 `--files test_load_config.properties` 将配置文件发布到集群上的 driver 节点和 executor 节点上，注意：
    * 该配置文件只有在 SparkContext 初始化完毕之后才可用（即该对象构建出来之后）
    * 同时，driver 上与 executor 上获取该配置文件的方式也不一样：
        * driver 上 `TestPropLoad.class.getResourceAsStream( "/test_load_config.properties" )`
        * executor 上 `new File(SparkFiles.get("test_load_config.properties"))`




