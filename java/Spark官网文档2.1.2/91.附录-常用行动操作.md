# 常用行动操作

## reduce(func)

* reduce 将 RDD 中元素前两个传给输入函数，产生一个新的 return 值，
    新产生的 return 值与 RDD 中下一个元素（第三个元素）组成两个元素，再被传给输入函数，直到最后只有一个值为止。

```python
data = sc.parallelize(range(10))
def reduce_func(x,y):
    print x,y
    return x+y
print data.reduce( reduce_func )
```

```bash
0 1
1 2
3 3
6 4
10 5
15 6
21 7
28 8
36 9
45
```

## collect()

* 将 rdd 中所有元素 load 到本地内存中，因此这里要求 rdd 中数据量比较小，单机内存可以容纳
    * 比较适合测试环境进行测试

## count()

* 获取 rdd 中元素的个数

## first()

* 获取 rdd 中第一个元素，类似与 `take(1)`

## take(n)

* 获取 rdd 中的 n 个元素，顺序不确定

## takeSample(withReplacement, num, [seed])

* 类似于转换操作中的 `sample` ， `takeSample = sample + take`

## takeOrdered(n, [ordering])

* 按照 ordering 提供的比较器，返回前 n 个元素

```python
data = sc.parallelize([1,2,3,3,2,1,1,23,4,123,123,12,3,12,3,1,23,1,23])
print data.takeOrdered( 15 )
```

```bash
[1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 12, 12, 23]
```

## saveAsTextFile(path)

* 将 RDD 以文本文件的格式存储到文件系统中

```python
data = sc.parallelize([1,2,3,3,2,1,1,23,4,123,123,12,3,12,3,1,23,1,23])
print data.saveAsTextFile( 'hdfs://172.18.1.39/user/xc_spark_test' )
```

```bash
$ ./bin/hadoop dfs -ls hdfs://172.18.1.39/user/
hdfs://172.18.1.39/user/xc_spark_test
...
```

## saveAsSequenceFile(path) (Java and Scala)

* 将 rdd 中的所有元素按照 SequenceFile 的文件格式写入本地文件系统、HDFS 或者任何其他的 Hadoop 支持的文件系统中，
    该函数只支持实现了 hadoop 的 Writable 的键值对 rdd。

## saveAsObjectFile(path) (Java and Scala)

* 将 rdd 中所有的元素使用 java 的 serialization 写入到文件中，该文件可以被 SparkContext.objectFile() 直接加载

## countByKey()

* 只支持键值对，返回 rdd 中各个 key 的个数

```python
data = [
    (2,3),
    (2,4),
    (1,3),
    (1,3),
    (5,3),
    (6,3),
]
print sc.parallelize(data).countByKey()
```

```bash
{1: 2, 2: 2, 5: 1, 6: 1}
```

## foreach(func)

* 使用传入的函数遍历 rdd 中的每一个元素

```python
def println(x):
    print x
    return x*10
sc.parallelize([1,2,3,2]).foreach(println)
```

```bash
1
2
3
2
```
