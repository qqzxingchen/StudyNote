# RDDs

## 生成 RDD

* Spark 提供了两种方式来生成 RDD
    * 序列化已有对象为 RDD
    * 加载文件内容生成 RDD

### 序列化已有对象为 RDD

* 序列化已有对象为 RDD
    * java
    ```java
    List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
    JavaRDD<Integer> distData = sc.parallelize(data);
    ```
    * python
    ```python
    data = [1, 2, 3, 4, 5]
    distData = sc.parallelize(data)
    ```

* 并行集合运算的一个重要参数就是要将数据集剪切成的分区数量，Spark 将会为每个分区运行一个任务。
    * 通常情况下，Spark将会默认地设置分区数量为 2到4倍CPU个数，但是我们也可以进行手动地调整，使用方法为：
        `sc.parallelize(data，10)`

### 加载文件内容生成 RDD

* Spark 同样可以从任何支持 Hadoop 接口的存储源创建RDDs，包括本地文件系统、HDFS、Cassandra、HBase、AmazonS3，
    同时 Spark 也支持 TextFile SequenceFile 以及 Hadoop 其他类型的 InputFormat

* 加载文件内容生成 RDD
    * java
    ```java
    JavaRDD<String> distFile = sc.textFile("data.txt");
    JavaRDD<String> distFile = sc.textFile("hdfs://172.18.1.39/apps/hive/warehouse/demo_btw_v1_3_label_dianzhichubanshe_by_gp.db/xxx.txt");
    ```
    * python
    ```python
    distFile = sc.textFile("data.txt")
    distFile = sc.textFile("hdfs://172.18.1.39/apps/hive/warehouse/demo_btw_v1_3_label_dianzhichubanshe_by_gp.db/xxx.txt")
    ```

* 注意事项
    * 无论是使用本地文件系统还是HDFS，都要保证指定路径在 Spark 的所有工作节点上都是可以访问的。
        这时，可以通过拷贝文件到所有工作节点的相应目录下，也可以创建一个共享文件系统来实现
    * 所有基于 Spark 文件输入的方法，包括 textFile ，都支持在目录，压缩文件和通配符上运行。
        例如，可以使用:
        * `textFile("/my/directory")`
        * `textFile("/my/directory/*.txt")`
        * `textFile("/my/directory/*.gz")`
    * `textFile` 方法同样支持传入预设的分区数量作为第二个参数。
        默认情况下， Spark 将会为每一个数据块创建一个分区（HDFS上一个数据块为 128 MB）。
        当然也可以传入一个更大的值来创建更多的分区，但是注意，传入的分区数不能小于数据块的数量

## 操作 RDD

* Spark 支持两种类型的 RDD 操作
    * 转化操作: 基于一个已有的 RDD 创建一个新的 RDD
    * 行动操作: 基于一个已有的 RDD 进行计算，并返回结果

* 在 Spark 中，所有的转化操作都是 lazy 的
    * 即它们直到需要进行行动操作时，才会对 RDD 进行实际的转换操作。否则，将只会把转换操作作为指令保存下来

* 默认情况下，每次对 RDD 进行操作时，每个已经转换的 RDD 都会重新进行计算（这样可以保证及时响应数据源中数据的变更）。
    但是，我们可以通过调用 persist 方法将某次 RDD 的转换结果保存到内存中，方便它的下次快速访问


### 基础操作

* 简单计算累加和的小程序
    * java
    ```java
    JavaRDD<String> lines = sc.textFile("data.txt");
    JavaRDD<Integer> lineLengths = lines.map(s -> s.length());
    int totalLength = lineLengths.reduce((a, b) -> a + b);
    lineLengths.persist(StorageLevel.MEMORY_ONLY());
    ```
    * python
    ```python
    lines = sc.textFile("data.txt")
    lineLengths = lines.map(lambda s: len(s))
    totalLength = lineLengths.reduce(lambda a, b: a + b)
    lineLengths.persist()
    ```

* Spark 的一个难点是在集群中执行代码时，变量和方法的作用范围和生命周期，修改范围之外的变量将可能会出现混淆。比如下列代码
    ```python
    counter = 0
    rdd = sc.parallelize(data)
    # Wrong: Don't do this!!
    def increment_counter(x):
        global counter
        counter += x
    rdd.foreach(increment_counter)
    print("Counter value: ", counter)
    ```
    * 以上代码的本意是计算 data 中所有元素的和，但是实际运行会发现，最终打印出来的 Counter value 为 0
    * 这是因为在每个节点上运行的计算任务都会包含一个闭包，该闭包中保存了所有可以访问的变量的`副本`，
        因此任一计算任务对于`同一变量`的修改都不会影响其它计算任务，也因此计算任务中的闭包的值也不会影响驱动任务节点中的变量的值
    * 针对以上代码，当在 foreach 函数中引用 counter 变量时，引用的实际上是被封入当前计算任务中的 counter 的副本，
        而非驱动任务节点中的 counter 变量。因此 foreach 中的任何对 counter 的修改，都不会影响到驱动任务节点上的 counter 变量的值
    * 一般来说，本地定义的方法这样的构造不应该被用来改变一些全局状态。Spark 并没有定义或保证对从封闭外引用的对象的突变行为。
        这样做的一些代码可能在本地模式下工作，但这是偶然的，这样的代码不会按预期在分布式模式下运行。
        如果需要全局聚合，请使用累加器。

### 键值对操作

* 大部分情况下， Spark 的转化、行动操作可以针对任意类型的；但是也有一些转化、行动操作，是专门针对于键值对类型的 RDD 的

* 简单实例
    * java
    ```java
    JavaRDD<String> lines = sc.textFile("data.txt");
    // 键值对相关的操作针对的是 java 中的 Tuple2 对象
    // 可以通过 tuple2._1() 以及 tuple2._2() 分别获取两个元素
    JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1));
    JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b);
    ```
    * python
    ```python
    lines = sc.textFile("data.txt")
    # 键值对相关的操作针对的是 python 中的 tuple 对象
    pairs = lines.map(lambda s: (s, 1))
    counts = pairs.reduceByKey(lambda a, b: a + b)
    ```

* 转换操作的 API 列表，请看 `90.附录-常用转换操作.md`



