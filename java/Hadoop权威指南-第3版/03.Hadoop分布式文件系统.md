
# Hadoop 分布式文件系统

* 当数据集的大小超过一台独立的物理计算机的存储能力时，就有必要对它进行分区并存储到若干台单独的计算机上
* 管理网络中跨多台计算机存储的文件系统称为分布式文件系统
* HDFS( Hadoop Distributed FileSystem ): Hadoop 的分布式系统



### HDFS 的设计

* HDFS 以流式数据访问模式来存储超大文件
    * 超大文件: 几百GB甚至几百TB大小的文件
    * 流式数据访问: 一次写入，多次读取。数据集被存储之后，通常要在该数据集上进行各种分析操作，且每次分析都将涉及该数据集的大部分数据乃至全部数据；
        在这种情况下，读取整个数据集的延迟要比读取第一条记录的延迟更重要
    * 商用硬件: HDFS 被设计成在遇到较高几率的节点故障时，可以持续运行并且不让用户察觉到明显的中断
    * 低时间延迟的数据访问: 要求低时间延迟的数据访问的应用(比如几十毫秒)不适合在 HDFS 上运行。
        因为 HDFS 是为了高数据吞吐量的应用做优化的，这可能会以提高时间延迟作为代价
    * 大量的小文件: namenode 将文件系统的元数据存储在内存中，因此该文件系统所存储的文件总数受限于 namenode 的内存容量。
        根据经验，每个文件、目录、数据块的存储信息大约占 150 字节
    * 多用户写入，任意修改文件: HDFS 中的文件可能只有一个 writer ，而且写操作总是将数据添加到文件的末尾。
        它不支持具有多个写入者的操作，也不支持在文件的任意位置进行修改。



### HDFS 的概念

* 数据块
    * 每个磁盘都有默认的数据块大小，这是进行数据读写的最小单位( 如 512B )；
        构建于单个磁盘上的文件系统则通过磁盘块来管理文件系统中的块，该文件系统的块大大小一般是磁盘块的整倍数( 如 4KB )
    * HDFS 的块要大的多，默认为 64 MB。这是为了最小化寻址开销
    * 与其他的文件系统类似，HDFS 上的文件也被划分为块大小的多个分块( chunk )，作为独立的存储单元。
        与其他文件系统不同的是， HDFS 中小于一个块大小的文件不会占据整个块的空间
    * 文件在写入 HDFS 时，会根据 HDFS 的 chunk 大小进行分块。而为了提高数据容错能力以及高可用性，
        HDFS 会将每个块复制到几个独立的机器上( 默认为3个 )，可以确保在块、磁盘、或者机器发生故障后数据不会丢失
    
```bash
# 获取文件系统中各个文件由那些块构成的
$ hadoop fsck / -files -blocks
```

* namenode 和 datanode
    * HDFS 有两类节点以 `管理者-工作者` 模式运行( 即一个 namenode 对应多个 datanode )
    * namenode 管理文件系统的命名空间，它维护着文件系统树以及整棵树内所有的文件和目录
        * 这些信息以两个文件的形式永久地保存在本地磁盘上: `命名空间镜像文件` 和  `编辑日志文件`
    * datanode 根据需要存储并检索数据块，并定期向 namenode 发送它们所存储的块的列表

* namenode 的容错
    * 没有 namenode ，文件系统将无法使用。
        * 如果运行 namenode 的服务器损坏，文件系统上的所有文件都将会丢失，因为我们并不知道该如何根据 datanode 的块重建文件
    * 因此我们需要对 namenode 实现容错， Hadoop 提供了两种机制
        * 机制一: 备份文件系统元数。Hadoop 可以通过配置使 namenode 在多个文件系统上进行元数据持久化，这些持久化操作是实时同步的，是原子操作。
            一般的场景，是持久化到一个本地磁盘的同时，还会同步到一个远程挂载的网络文件系统( NFS )
        * 机制二: 运行一个辅助 namenode ( 但它并不能用作 namenode，正常情况下并不对外提供服务 )，
            它会定期通过编辑日志合并命名空间镜像，并在 namenode 发生故障时启用。
            但是由于它总是会滞后于 namenode ，因此在 namenode 节点失效时，会丢失一部分数据

* 联邦 HDFS
    * namenode 在内存中保存文件系统的每个文件和每个数据块的引用关系，这对于一个拥有大量文件的超大集群来说，内存将成为限制系统横向扩展的瓶颈。
    * 在 2.x 发行版本系列中引入的联邦 HDFS 允许系统通过添加 namenode 实现扩展，其中每个 namenode 管理文件系统命名空间中的一部分

* HDFS 的高可用性
    * 可以通过下面的方式避免数据的丢失
        * 在多个文件系统中备份 namenode
        * 通过备用 namenode 创建监测点
    * 但是上述方式依然无法实现文件系统的高可用性， namenode 依然存在单点失效(SPOF)的问题。
        * 如果 namenode 失效，则所有的客户端，包括 MapReduce 任务均无法读、写或列文件，因为 namenode 是唯一存储文件到数据块映射的地方
    * Hadoop 2.x 发行版针对 SPOF 的问题增加了对高可用性(HA)的支持
        * 通过使用一对 `活动-备用 namenode` 来实现 HA: 当活动 namenode 失效，备用 namenode 将会接管它的任务并对外服务，不会有明显中断
    * 要实现 HA ，则需要在架构上进行调整
        * namenode 之间需要通过高可用的共享存储实现编辑日志的共享(比如构建于 ZooKeeper 上的 BookKeeper)
            当备用 namenode 接管工作之后，它将通读共享编辑日志直到末尾，实现与活动 namenode 状态的同步
        * datanode 需要同时向两个 namenode 发送数据块处理报告
        * 客户端需要使用特定的机制来察觉 namenode 的失效，并自动进行切换
    * 在活动 namenode 失效之后，备用 namenode 可以快速(几十秒，实测大约1分钟)实现任务接管
    * 故障切换与规避
        * 一个专用于管理将活动 namenode 转移为备用 namenode 的转换过程



### 命令行接口

* 下面是几个简单的命令

```bash
# 将本地文件系统的文件复制到 HDFS
$ hadoop fs -copyFromLocal xxx.txt hdfs://localhost:8020/user/xc/xxx.txt

# 将 HDFS 的文件复制到本地
$ hadoop fs -copyToLocal hdfs://localhost:8020/user/xc/xxx.txt xxx.txt

# 创建目录
$ hadoop fs -mkdir /user/xc/books

# 列出目录
# 列出的信息中，按列分别为： 文件模式、文件备份数、用户组、用户、大小、最后修改的年月日、最后修改的时分、文件/目录的绝对路径
$ hadoop fs -ls /user/xc
> -rw-r--r--   3 root      hadoop       2117 2016-11-09 10:10 /user/xc/1.txt
> drwxrwxrwx   - bfdhadoop hadoop          0 2017-06-09 11:07 /user/xc/books
```



### Hadoop 文件系统

* Hadoop 有一个抽象的文件系统概念， HDFS 只是其中一个实现
    * java 抽象类 `org.apache.hadoop.fs.FileSystem` 定义了 Hadoop 中的一个文件系统接口，并具有几个具体实现

| 文件系统 | URI 方案 | Java 实现(均包含在 org.apache.hadoop 包中) | 描述 |
|:---|:---|:---|:---|
| Local | file | fs.LocalFileSystem | 使用了客户端校验和的磁盘文件系统。没有使用校验和的本地磁盘文件系统 RawLocalFileSystem |
| HDFS | hdfs | hdfs.DistributedFileSystem | Hadoop 分布式文件系统。将 HDFS 设计成与 MapReduce 结合使用，可以实现高性能 |
| HFTP | hftp | hdfs.hftpFileSystem | 在 HTTP 上提供对 HDFS 只读访问的文件系统，通常和 distcp 结合使用以实现在运行不同版本的 HDFS 之间共享数据 |
| HSFTP | hsftp | hdfs.HsftpFileSystem | 在 HTTPS 上提供对 HDFS 只读访问的文件系统 |
| WebHDFS | Webhdfs | hdfs.web.WebHdfsFileSystem | 基于 HTTP ，对 HDFS 提供安全读写访问的文件系统。 WebHDFS 是为了替代 HFTP 和 HSFTP 而构建的 |
| HAR | har | fs.HarFileSystem | 构建于其他文件系统之上用于文件存档的文件系统。Hadoop 存档文件系统通常用于需要将 HDFS 中的文件进行存档时，以减少 namenode 内存的使用 |
| hfs | kfs | fs.kfs.kosmosFileSystem | CloudStore 是类似于 HDFS 或是谷歌的 GFS 的文件系统 |
| FTP | ftp | fs.ftp.FTPFileSystem | 由 FTP 服务器支持的文件系统 |
| S3(原生) | S3n | fs.s3native.NativeS3FileSystem | Amazon S3 支持的文件系统 |
| S3(基于块) | S3 | fs.sa.S3FileSystem | 由 Amazon S3 支持的文件系统，以块格式存储文件 |
| 分布式 RAID | hdfs | hdfs.DistributedRaidFileSystem | RAID 版本的 HDFS 是为了存档而设计的。针对 HDFS 中的每个文件，创建一个(更小的)校验文件，并允许 HDFS 的数据副本由3将为2，由此减少 25%~35% 的存储空间，但是数据丢失的概率保持不变 |
| View | viewfs | viewfs.ViewFileSystem | 针对其他 Hadoop 文件系统挂载的客户端标，通常用于联邦 HDFS 创建挂载点 |

* 尽管运行的 MapReduce 程序可以访问任何文件系统，但是在处理大数据集时，还是选择一个有数据本地优化的分布式文件系统更好，如 HDFS





